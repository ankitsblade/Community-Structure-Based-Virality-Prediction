# ğŸ§µ Community-Structure-Based Virality Prediction

This project predicts whether a Reddit post becomes **micro-viral** or **viral** using:

* Multi-subreddit cascade extraction
* Community structure analysis via user-graph Louvain detection
* Cascade structural + temporal features
* Classical ML models
* Graph Neural Networks (GraphSAGE)
* Cross-subreddit and early-cascade prediction

The pipeline supports **two data sources**:

1. **Arctic Shift Reddit Comment/Submission Dump** (research dump)
2. **Live PRAW API collection** (for fresh cascades)

---

# ğŸ“ Final Project Structure

```
Community-Structure-Based-Virality-Prediction/
â”‚
â”œâ”€â”€ collect_cascades.py              # Reddit data collection using PRAW
â”œâ”€â”€ data_util/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cascades.py                  # Submission â†’ cascade extraction
â”‚   â”œâ”€â”€ collect_cascades_pushshift.py
â”‚   â”œâ”€â”€ collect_cascades.py          # Multi-subreddit cascade loader
â”‚   â””â”€â”€ reddit_client.py             # PRAW client + API helpers
â”‚
â”œâ”€â”€ dataset/                         # Arctic Shift Dataset (2023-11 reddit data)
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ features.py                  # Structural, temporal, community features
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ classicml/                   # Classical ML visuals
â”‚   â”œâ”€â”€ gnn/                         # GNN visuals
â”‚   â”œâ”€â”€ figures_old/
â”‚   â””â”€â”€ reddit/
â”‚
â”œâ”€â”€ misc/
â”‚   â””â”€â”€ graphviz.sh                  # To Download graphviz on non-sudo compute
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gnn.py                       # GraphSAGE + GNN dataset builder
â”‚   â””â”€â”€ models_ml.py                 # Logistic Regression + Random Forest
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Experiments.ipynb            # Scratch experiments
â”‚
â”œâ”€â”€ Training/
â”‚   â”œâ”€â”€ run_gnn.py                   # GNN pipeline runner
â”‚   â””â”€â”€ run_pipeline.py              # Classical ML pipeline runner
â”‚
â”œâ”€â”€ util/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_io.py                   # Save/load utilities
â”‚   â”œâ”€â”€ graphs.py                    # User graph + Louvain communities
â”‚   â””â”€â”€ metrics.py                   # ROC-AUC helper
â”‚
â”œâ”€â”€ viz/
â”‚   â””â”€â”€ visuals.py                   # All visualizations
â”‚
â”‚â”€â”€ config.py                        # Subreddit list + constants
â”‚â”€â”€ logger.py                        # Logging setup
â”‚â”€â”€ microviral.log                   # Log file
â”‚â”€â”€ README.md
â”‚â”€â”€ pyproject.toml
â”‚â”€â”€ .env
â”‚â”€â”€ .gitignore
â””â”€â”€ uv.lock
```

---

# ğŸ§Š Data Sources Used

### **1ï¸âƒ£ Arctic Shift Reddit Dump**

This project uses the **Arctic Shift Reddit dataset**, a research-focused archived dump containing:

* Reddit submissions
* Reddit comments
* Author IDs
* Subreddit metadata
* Timestamps

It provides the **core comment trees** for building cascade structures.

### **2ï¸âƒ£ PRAW API (Python Reddit API Wrapper)**

To supplement or refresh data, PRAW is used for:

* Fetching recent submissions
* Getting full comment trees
* Extracting reply chains
* Collecting subreddit-specific cascades

The PRAW client is defined in:

```
data_util/reddit_client.py
```

You may run only Arctic Shift data, only PRAW data, or combine both.

---

# ğŸš€ How to Run

This section explains how to reproduce your dataset, extract features, train models, and generate all figures.

---

## 1ï¸âƒ£ Install environment using **uv**

```bash
uv sync
```

This:

* Creates `.venv/`
* Installs all dependencies from `pyproject.toml`
* Ensures consistent environment

No manual `pip install` required.

---

## 2ï¸âƒ£ Add Reddit API Credentials (PRAW Use Only)

Create `.env`:

```
CLIENT_ID=xxxx
CLIENT_SECRET=xxxx
USERNAME=xxxx
PASSWORD=xxxx
USER_AGENT=microviral:v1.0
```

If using only the Arctic Shift dump, this step is optional.

---

## 3ï¸âƒ£ Obtain or Generate the Dataset

### Option A â€” Use Arctic Shift Reddit Dump

Place the processed/parquet dump inside:

```
dataset/
```

The pipeline automatically loads it.

### Option B â€” Collect Fresh Cascades Using PRAW

```bash
python collect_cascades.py
```

This produces:

```
dataset/nodes_multi.parquet
```

Containing:

* All nodes (submissions + comments)
* Parent/child edges
* Author IDs
* Subreddit labels
* Timestamps, scores, depths

---

## 4ï¸âƒ£ Run the Classical ML Pipeline

```bash
python Training/run_pipeline.py
```

This performs:

* User graph construction
* Louvain community detection
* Modularity computation
* Feature extraction
* Virality labeling
* Training LogReg + RandomForest
* Per-subreddit metrics
* Exhaustive visualizations

Results saved to:

```
figures/classicml/
dataset/
```

---

## 5ï¸âƒ£ Run the GNN Pipeline

```bash
python Training/run_gnn.py
```

This runs:

* GraphSAGE training
* Per-subreddit GNN models
* Cross-subreddit generalization
* Feature ablation studies

Outputs saved to:

```
figures/gnn/
results/gnn/
```

---

# ğŸ§© Summary of the Pipeline

This project performs:

1. Multi-subreddit cascade extraction (Arctic Shift + PRAW)
2. User-graph construction (reply network)
3. Louvain community structure detection
4. Feature engineering:

   * Depth, branching, entropy
   * Temporal bursts (time to 5, time-normalized flow)
   * Community attributes
5. Global virality labeling
6. Classical ML baselines
7. Graph Neural Network modeling (GraphSAGE)
8. Cross-subreddit + per-subreddit experiments
9. Feature ablations
10. Automatic figure generation for posters/papers

---

## Poster [Submitted for CS4222 Social Computing]

![alt text](<SocialComp Poster-1.png>)